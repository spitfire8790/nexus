import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
import math
from urllib.parse import quote
from supabase import create_client
import os
from datetime import datetime

class PropertyScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.getsoldprice.com.au/'
        }
        self.session = requests.Session()

    def clean_land_size(self, land_size):
        """Clean land size string to extract just the number"""
        if land_size == "N/A":
            return land_size
        match = re.search(r'(\d+(?:\.\d+)?)', land_size)
        return match.group(1) if match else "N/A"

    def extract_property_details(self, panel, suburb):
        """Extract details from a single property panel"""
        try:
            # Check if this is a property panel by looking for house-image
            house_image = panel.find('div', class_='house-image')
            if not house_image:
                return None
                
            # Extract address
            title = panel.find('h4', class_='panel-title')
            address_link = title.find('a') if title else None
            if not address_link:
                return None
            address = address_link.text.strip()
            
            # Extract price and date
            body = panel.find('div', class_='panel-body')
            price = "N/A"
            sold_date = "N/A"
            
            if body:
                price_code = body.find('code')
                if price_code:
                    price = price_code.text.strip()
                
                date_text = body.find('p')
                if date_text and 'Sold' in date_text.text:
                    sold_date = date_text.text.split(' on ')[-1].strip()
            
            # Extract features
            features = panel.find('p', class_='features')
            property_type = "N/A"
            beds = baths = cars = "N/A"
            
            if features:
                # Property type
                type_span = features.find('span', class_='label-info')
                if type_span:
                    property_type = type_span.text.strip()
                
                # Bed, bath, car
                for span in features.find_all('span'):
                    if span.find('i', class_='bedrooms'):
                        beds = span.text.strip()
                    elif span.find('i', class_='bathrooms'):
                        baths = span.text.strip()
                    elif span.find('i', class_='car_spaces'):
                        cars = span.text.strip()
            
            # Extract land size
            house_desc = panel.find('div', class_='house-desc')
            land_size = "N/A"
            if house_desc:
                for div in house_desc.find_all('div'):
                    if div.text.startswith('Land Size:'):
                        land_size = div.text.replace('Land Size:', '').strip()
            
            # Clean land size to just the number
            land_size = self.clean_land_size(land_size)
            
            return {
                'suburb': suburb,
                'address': address,
                'price': price,
                'sold_date': sold_date,
                'property_type': property_type,
                'bedrooms': beds,
                'bathrooms': baths,
                'parking': cars,
                'land_size': land_size
            }
            
        except Exception as e:
            print(f"Error extracting property details: {str(e)}")
            return None

    def scrape_page(self, url, suburb):
        """Scrape a single page of property listings"""
        try:
            print(f"\nFetching URL: {url}")
            response = self.session.get(url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            panels = []
            for panel in soup.find_all('div', class_='panel-primary'):
                if panel.find('div', class_='house-image'):
                    panels.append(panel)
            
            print(f"Found {len(panels)} property panels on this page")
            
            properties = []
            for panel in panels:
                property_details = self.extract_property_details(panel, suburb)
                if property_details:
                    properties.append(property_details)
                    print(f"Successfully extracted property: {property_details['address']}")
            
            return soup, properties
            
        except Exception as e:
            print(f"Error scraping page {url}: {str(e)}")
            return None, []

    def scrape_suburb(self, state, postcode, suburb, year_min=2023, year_max=2024):
        """Scrape all pages for a given suburb with year filter"""
        all_properties = []
        page = 1
        total_pages = None
        
        # URL encode suburb name
        encoded_suburb = quote(suburb)
        
        # Get first page and total results
        if page == 1:
            url = f"https://www.getsoldprice.com.au/sold/list/state/{state}/postcode/{postcode}/suburb/{encoded_suburb}/?type=all&ymin={year_min}&ymax={year_max}&bmin=0&bmax=0&pmin=0&pmax=0&sort=date&kw="
            soup, properties = self.scrape_page(url, suburb)
            
            if soup:
                total_results = self.get_total_results(soup)
                if total_results:
                    total_pages = math.ceil(total_results / 12)
                    print(f"\nFound {total_results} total properties across {total_pages} pages")
            
            if properties:
                all_properties.extend(properties)
        
        # Continue with remaining pages
        while True:
            if total_pages and page >= total_pages:
                print("\nReached last page")
                break
                
            page += 1
            url = f"https://www.getsoldprice.com.au/sold/list/p/{page}/state/{state}/postcode/{postcode}/suburb/{encoded_suburb}/sort/date/type/all/ymin/{year_min}/ymax/{year_max}/"
            
            print(f"\nScraping page {page} of {total_pages or 'unknown'}...")
            _, properties = self.scrape_page(url, suburb)
            
            if not properties:
                print(f"No properties found on page {page}, stopping.")
                break
            
            all_properties.extend(properties)
            print(f"Total properties scraped so far: {len(all_properties)}")
            
            time.sleep(random.uniform(2, 4))
        
        return pd.DataFrame(all_properties)

    def get_total_results(self, soup):
        """Extract total number of results from the page"""
        try:
            # Look for the results count text
            results_text = soup.find('div', class_='panel-heading').text
            if results_text:
                # Extract number from text like "Found 123 properties"
                match = re.search(r'Found (\d+) properties', results_text)
                if match:
                    return int(match.group(1))
            return 0
        except Exception as e:
            print(f"Error getting total results: {str(e)}")
            return 0

def main():
    # Supabase configuration - replace these with your actual values
    supabase_url = "https://bgrbegqeoyolkrxjebho.supabase.co"  # Replace with your URL
    supabase_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJncmJlZ3Flb3lvbGtyeGplYmhvIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTczMTgwOTE2MiwiZXhwIjoyMDQ3Mzg1MTYyfQ.RUO1z5vTRzytsJYdeBQNJNdt7fTh1shLygxtSpuCM1I"  # Replace with your service role key
    
    # Comment out the environment variable check
    # if not supabase_url or not supabase_key:
    #     raise ValueError("Please set SUPABASE_URL and SUPABASE_KEY environment variables")
    
    supabase = create_client(supabase_url, supabase_key)
    
    # Use full path to suburbs.csv
    csv_path = r"C:\Users\James\Desktop\nexus\public\data\sale prices\suburbs.csv"
    suburbs_to_scrape = pd.read_csv(csv_path)
    
    scraper = PropertyScraper()
    all_data = []
    
    # Scrape each suburb
    for _, suburb_info in suburbs_to_scrape.iterrows():
        print(f"\nScraping data for {suburb_info['suburb']}...")
        df = scraper.scrape_suburb(
            state='NSW',
            postcode=str(suburb_info['postcode']),
            suburb=suburb_info['suburb'],
            year_min=2023,
            year_max=2024
        )
        if not df.empty:
            all_data.append(df)
    
    # Combine all data
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # Clean up the data
        combined_df = combined_df.replace('N/A', pd.NA)
        combined_df = combined_df.dropna(how='all')
        
        # Clean price column
        if 'price' in combined_df.columns:
            combined_df['price'] = combined_df['price'].str.replace('$', '').str.replace(',', '')
            combined_df['price'] = pd.to_numeric(combined_df['price'], errors='coerce')
        
        # Convert dates
        if 'sold_date' in combined_df.columns:
            combined_df['sold_date'] = pd.to_datetime(combined_df['sold_date'], errors='coerce')
        
        # Convert numeric columns
        for col in ['bedrooms', 'bathrooms', 'parking', 'land_size']:
            if col in combined_df.columns:
                combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')
        
        # Add timestamp for when the data was collected
        combined_df['collected_at'] = datetime.now().isoformat()
        
        # Convert DataFrame to list of dictionaries for Supabase
        records = combined_df.to_dict('records')
        
        # Upload to Supabase in batches to avoid timeout
        batch_size = 100
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            try:
                response = supabase.table('nsw_property_sales').upsert(
                    batch,
                    on_conflict='address,sold_date'
                ).execute()
                print(f"Uploaded batch {i//batch_size + 1} of {math.ceil(len(records)/batch_size)}")
            except Exception as e:
                print(f"Error uploading batch {i//batch_size + 1}: {str(e)}")
        
        print(f"\nFinal Results:")
        print(f"Scraped {len(combined_df)} total properties")
        
        print("\nProperties by Suburb:")
        print(combined_df['suburb'].value_counts())
        
        print("\nProperty Types by Suburb:")
        print(pd.crosstab(combined_df['suburb'], combined_df['property_type']))
        
        print("\nDate Range Summary:")
        print(f"Earliest Date: {combined_df['sold_date'].min()}")
        print(f"Latest Date: {combined_df['sold_date'].max()}")

if __name__ == "__main__":
    main()